"""
GENESIS: Ultimate Paradigm Comparison
Author: GENESIS Project
Date: 2026-01-04

ê·¼ë³¸ì  ì§ˆë¬¸ì— ëŒ€í•œ ë‹µ:
    "What is the difference between autopoietic learning and ML?"

ë¹„êµ ì°¨ì›:
    1. Objective: External vs Internal
    2. Mechanism: Optimization vs Organization
    3. Criterion: Loss/Reward vs Coherence
    4. Causality: Linear vs Circular
    5. Adaptation: Parameter tuning vs Structural drift

ì‹¤í—˜:
    - Autopoietic Population
    - Supervised Learning (SGD)
    - Reinforcement Learning
    - Hebbian Learning
    - Random Baseline

ì¸¡ì •:
    - Survival rate
    - Organizational coherence
    - Adaptation capacity
    - Structural changes
"""

import sys
import os
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', 'core'))

import numpy as np
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
from collections import defaultdict, deque

from autopoietic_population import (
    AutopoeticPopulation,
    PerturbationField
)


class MLPopulation:
    """
    ML ê¸°ë°˜ ê°œì²´êµ° (ë¹„êµìš©)

    ì°¨ì´ì :
        - External fitness function
        - Gradient-based optimization
        - Fixed structure
    """

    def __init__(self, learning_type: str, population_size: int = 10):
        """
        Args:
            learning_type: 'supervised', 'rl', 'hebbian', 'random'
            population_size: ê°œì²´ ìˆ˜
        """
        self.learning_type = learning_type
        self.entities = []

        # ê°œì²´ ìƒì„±
        for i in range(population_size):
            entity = {
                'W': np.random.randn(20, 20) * 0.2,
                'state': np.zeros(20),
                'fitness': 0.5,
                'age': 0
            }
            self.entities.append(entity)

        self.generation = 0
        self.avg_fitness_history = deque(maxlen=1000)

    def step(self, perturbation_field: PerturbationField) -> dict:
        """í•œ ìŠ¤í… ì‹¤í–‰"""
        self.generation += 1

        # í–‰ë™ ìƒì„±
        actions = []
        for entity in self.entities:
            # ë‚´ë¶€ ì—­í•™
            entity['state'] = np.tanh(np.dot(entity['W'], entity['state']))
            action = entity['state'][:3]
            actions.append(action)

        # êµë€ ë°›ê¸°
        perturbations = perturbation_field.step(actions)

        # í•™ìŠµ ë° fitness í‰ê°€
        fitnesses = []

        for i, entity in enumerate(self.entities):
            perturbation = perturbations[i] if i < len(perturbations) else np.random.randn(20) * 0.3

            # "fitness" = ì–¼ë§ˆë‚˜ êµë€ì„ ì˜ ë³´ìƒí•˜ëŠ”ê°€ (ì™¸ë¶€ ê¸°ì¤€!)
            compensation_quality = -np.linalg.norm(entity['state'] + perturbation)
            fitness = 1.0 / (1.0 + abs(compensation_quality))

            # í•™ìŠµ
            if self.learning_type == 'supervised':
                # Target = êµë€ì˜ ë°˜ëŒ€ (ì™¸ë¶€ ëª©í‘œ!)
                target = -perturbation
                error = entity['state'] - target[:20]
                entity['W'] -= 0.01 * np.outer(entity['state'], error)

            elif self.learning_type == 'rl':
                # Reward = fitness (ì™¸ë¶€ ë³´ìƒ!)
                if fitness > entity['fitness']:
                    entity['W'] += 0.01 * np.outer(entity['state'], entity['state'])

            elif self.learning_type == 'hebbian':
                # ìƒê´€ê´€ê³„ ê¸°ë°˜
                entity['W'] += 0.01 * np.outer(entity['state'], entity['state'])

            # Randomì€ í•™ìŠµ ì•ˆ í•¨

            entity['fitness'] = fitness
            entity['age'] += 1
            fitnesses.append(fitness)

        self.avg_fitness_history.append(np.mean(fitnesses))

        return {
            'generation': self.generation,
            'avg_fitness': np.mean(fitnesses)
        }

    def get_summary(self) -> dict:
        return {
            'generation': self.generation,
            'avg_fitness': np.mean(list(self.avg_fitness_history)) if len(self.avg_fitness_history) > 0 else 0
        }


def run_ultimate_comparison(n_generations: int = 500) -> dict:
    """
    ìµœì¢… íŒ¨ëŸ¬ë‹¤ì„ ë¹„êµ

    Returns:
        results: ëª¨ë“  íŒ¨ëŸ¬ë‹¤ì„ ê²°ê³¼
    """
    print("=" * 70)
    print("ULTIMATE PARADIGM COMPARISON")
    print("=" * 70)
    print("\nQuestion: What makes Autopoiesis fundamentally different?")
    print("Answer: We'll find out...\n")

    paradigms = [
        ('Autopoietic', 'autopoietic'),
        ('Supervised (SGD)', 'supervised'),
        ('Reinforcement Learning', 'rl'),
        ('Hebbian Learning', 'hebbian'),
        ('Random Baseline', 'random')
    ]

    results = {}

    for name, ptype in paradigms:
        print(f"\n{'='*70}")
        print(f"Testing: {name}")
        print(f"{'='*70}")

        # êµë€ì¥ ìƒì„±
        field = PerturbationField(field_size=20, turbulence=0.3)

        # ê°œì²´êµ° ìƒì„±
        if ptype == 'autopoietic':
            population = AutopoeticPopulation(
                initial_population=10,
                max_population=30,
                reproduction_threshold=0.7,
                mutation_rate=0.1
            )
        else:
            population = MLPopulation(ptype, population_size=10)

        # ì§„í™”/í•™ìŠµ
        history = []

        for gen in range(n_generations):
            stats = population.step(field)

            if ptype == 'autopoietic':
                history.append({
                    'gen': gen,
                    'population': stats['population'],
                    'coherence': stats['avg_coherence'],
                    'fitness': stats['avg_fitness']
                })

                if gen % 100 == 0:
                    print(f"  Gen {gen:3d} | Pop: {stats['population']:2d} | "
                          f"Coherence: {stats['avg_coherence']:.3f} | "
                          f"Fitness: {stats['avg_fitness']:.3f}")
            else:
                history.append({
                    'gen': gen,
                    'fitness': stats['avg_fitness']
                })

                if gen % 100 == 0:
                    print(f"  Gen {gen:3d} | Fitness: {stats['avg_fitness']:.3f}")

        # ê²°ê³¼ ì €ì¥
        results[name] = {
            'type': ptype,
            'history': history,
            'summary': population.get_summary()
        }

    return results


def analyze_paradigm_differences(results: dict) -> dict:
    """íŒ¨ëŸ¬ë‹¤ì„ ì°¨ì´ ë¶„ì„"""

    print(f"\n{'='*70}")
    print("PARADIGM ANALYSIS")
    print(f"{'='*70}\n")

    analysis = {}

    # ë¹„êµ í‘œ
    print(f"{'Paradigm':<30} | {'Final Metric':>15} | {'Key Difference':>20}")
    print("-" * 70)

    for name, data in results.items():
        if data['type'] == 'autopoietic':
            final_metric = data['summary']['avg_coherence']
            metric_name = "Coherence"
            key_diff = "Internal organization"
        else:
            final_metric = data['summary']['avg_fitness']
            metric_name = "Fitness"
            key_diff = "External optimization"

        print(f"{name:<30} | {metric_name}: {final_metric:>7.3f} | {key_diff:>20}")

        analysis[name] = {
            'final_metric': final_metric,
            'metric_name': metric_name,
            'key_difference': key_diff
        }

    # ê·¼ë³¸ì  ì°¨ì´ ì„¤ëª…
    print(f"\n{'='*70}")
    print("FUNDAMENTAL DIFFERENCES")
    print(f"{'='*70}\n")

    print("Autopoietic vs All ML Paradigms:\n")

    comparison_table = """
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Dimension           â”‚ ML Paradigms       â”‚ Autopoietic          â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚ Objective           â”‚ External (loss/R)  â”‚ Internal (coherence) â”‚
    â”‚ Mechanism           â”‚ Optimization       â”‚ Organization         â”‚
    â”‚ Learning            â”‚ Gradient/Hebbian   â”‚ Structural drift     â”‚
    â”‚ Criterion           â”‚ Performance        â”‚ Self-maintenance     â”‚
    â”‚ Causality           â”‚ Linear (Iâ†’Oâ†’L)    â”‚ Circular (closure)   â”‚
    â”‚ Structure           â”‚ Fixed architecture â”‚ Mutable topology     â”‚
    â”‚ Goal                â”‚ Predefined         â”‚ Self-generated       â”‚
    â”‚ Evaluation          â”‚ External metric    â”‚ Intrinsic coherence  â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    """

    print(comparison_table)

    return analysis


def plot_ultimate_comparison(results: dict,
                            save_path: str = '../../results/ultimate_comparison.png'):
    """ìµœì¢… ë¹„êµ ì‹œê°í™”"""

    fig = plt.figure(figsize=(16, 12))
    gs = fig.add_gridspec(3, 2, hspace=0.3, wspace=0.3)

    colors = {
        'Autopoietic': '#2E86AB',
        'Supervised (SGD)': '#A23B72',
        'Reinforcement Learning': '#F18F01',
        'Hebbian Learning': '#C73E1D',
        'Random Baseline': '#6C757D'
    }

    # Plot 1: Fitness/Coherence Over Time
    ax1 = fig.add_subplot(gs[0, :])

    for name, data in results.items():
        history = data['history']
        gens = [h['gen'] for h in history]

        if data['type'] == 'autopoietic':
            metrics = [h['coherence'] for h in history]
            label = f"{name} (Coherence)"
        else:
            metrics = [h['fitness'] for h in history]
            label = f"{name} (Fitness)"

        ax1.plot(gens, metrics, label=label, color=colors[name],
                linewidth=2, alpha=0.8)

    ax1.set_xlabel('Generation', fontsize=12)
    ax1.set_ylabel('Metric Value', fontsize=12)
    ax1.set_title('Evolution of Metrics Over Time', fontsize=14, fontweight='bold')
    ax1.legend(fontsize=10, loc='best')
    ax1.grid(alpha=0.3)
    ax1.set_ylim([0, 1])

    # Plot 2: Population Size (Autopoietic only)
    ax2 = fig.add_subplot(gs[1, 0])

    auto_data = results['Autopoietic']
    gens = [h['gen'] for h in auto_data['history']]
    pops = [h['population'] for h in auto_data['history']]

    ax2.plot(gens, pops, color=colors['Autopoietic'], linewidth=2)
    ax2.fill_between(gens, pops, alpha=0.3, color=colors['Autopoietic'])
    ax2.set_xlabel('Generation', fontsize=11)
    ax2.set_ylabel('Population Size', fontsize=11)
    ax2.set_title('Autopoietic Population Dynamics', fontsize=12, fontweight='bold')
    ax2.grid(alpha=0.3)

    # Plot 3: Final Metrics Comparison
    ax3 = fig.add_subplot(gs[1, 1])

    paradigm_names = list(results.keys())
    final_values = []

    for name in paradigm_names:
        data = results[name]
        if data['type'] == 'autopoietic':
            final_values.append(data['summary']['avg_coherence'])
        else:
            final_values.append(data['summary']['avg_fitness'])

    bars = ax3.bar(range(len(paradigm_names)), final_values,
                   color=[colors[n] for n in paradigm_names], alpha=0.7)

    # Autopoietic ê°•ì¡°
    bars[0].set_edgecolor('black')
    bars[0].set_linewidth(3)

    ax3.set_xticks(range(len(paradigm_names)))
    ax3.set_xticklabels([n.replace(' (SGD)', '').replace(' Learning', '')
                         for n in paradigm_names],
                        rotation=15, ha='right', fontsize=9)
    ax3.set_ylabel('Final Metric Value', fontsize=11)
    ax3.set_title('Final Performance Comparison', fontsize=12, fontweight='bold')
    ax3.grid(axis='y', alpha=0.3)
    ax3.set_ylim([0, 1])

    # Plot 4: Paradigm Characteristics (í…ìŠ¤íŠ¸)
    ax4 = fig.add_subplot(gs[2, :])
    ax4.axis('off')

    characteristics_text = """
    PARADIGM CHARACTERISTICS

    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
    â•‘                                                                        â•‘
    â•‘  AUTOPOIETIC (GENESIS)                                                â•‘
    â•‘  âœ“ Internal coherence (NO external objective)                         â•‘
    â•‘  âœ“ Circular causality (organization produces itself)                  â•‘
    â•‘  âœ“ Structural drift (NO gradient descent)                             â•‘
    â•‘  âœ“ Self-generated norms (autonomous)                                  â•‘
    â•‘                                                                        â•‘
    â•‘  ALL ML PARADIGMS (Supervised, RL, Hebbian)                           â•‘
    â•‘  âœ— External optimization (loss/reward minimization)                   â•‘
    â•‘  âœ— Linear causality (input â†’ process â†’ output â†’ learn)               â•‘
    â•‘  âœ— Parameter optimization (gradient/correlation)                      â•‘
    â•‘  âœ— Predefined goals (fitness function)                                â•‘
    â•‘                                                                        â•‘
    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    KEY INSIGHT:
    Autopoietic learning is not "better" or "worse" than ML.
    It is FUNDAMENTALLY DIFFERENT - a different kind of intelligence.

    ML optimizes external objectives â†’ Performance
    Autopoiesis maintains internal organization â†’ Viability
    """

    ax4.text(0.5, 0.5, characteristics_text, fontsize=10, ha='center', va='center',
            family='monospace',
            bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.2))

    plt.suptitle('ULTIMATE PARADIGM COMPARISON: Autopoiesis vs ML',
                fontsize=16, fontweight='bold')

    # ì €ì¥
    import os
    os.makedirs(os.path.dirname(save_path), exist_ok=True)
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    print(f"\nFigure saved: {save_path}")
    plt.close()


if __name__ == "__main__":
    print("""
    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
    â•‘                                                                   â•‘
    â•‘              ULTIMATE PARADIGM COMPARISON                        â•‘
    â•‘                                                                   â•‘
    â•‘  Question: What makes GENESIS fundamentally different from ML?   â•‘
    â•‘                                                                   â•‘
    â•‘  Autopoietic vs Supervised vs RL vs Hebbian vs Random           â•‘
    â•‘                                                                   â•‘
    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)

    # ì‹¤í—˜ ì‹¤í–‰
    results = run_ultimate_comparison(n_generations=500)

    # ë¶„ì„
    analysis = analyze_paradigm_differences(results)

    # ì‹œê°í™”
    plot_ultimate_comparison(results)

    print("\n" + "=" * 70)
    print("ULTIMATE COMPARISON COMPLETE!")
    print("=" * 70)

    print("\nğŸ¯ FINAL ANSWER:")
    print("\n  GENESIS (Autopoietic) is not 'better ML'.")
    print("  It is a DIFFERENT KIND of system:")
    print("    - From optimization to organization")
    print("    - From external goals to intrinsic viability")
    print("    - From parameter tuning to structural evolution")
    print("\n  This is the paradigm shift we sought.")
