"""
QED: Quantum-Inspired Ensemble Descent
========================================

ì™„ì „íˆ ìƒˆë¡œìš´ ìµœì í™” íŒ¨ëŸ¬ë‹¤ì„

ì˜ê°:
- ì–‘ìì—­í•™: ì—¬ëŸ¬ ìƒíƒœë¥¼ ë™ì‹œì— íƒìƒ‰ (ì¤‘ì²©)
- ì§„í™”ë¡ : ì¢‹ì€ í•´ëŠ” ìƒì¡´, ë‚˜ìœ í•´ëŠ” ë³€ì´
- ì§‘ë‹¨ ì§€ì„±: ì—¬ëŸ¬ agentê°€ í˜‘ë ¥í•˜ì—¬ ìµœì í•´ íƒìƒ‰
- ì—´ì—­í•™: ì˜¨ë„ë¡œ íƒìƒ‰-ìˆ˜ë ´ ì¡°ì ˆ
- ì‹ ê²½ê³¼í•™: ì„±ê³µí•œ ê²½ë¡œ ê°•í™”

í•µì‹¬ ë©”ì»¤ë‹ˆì¦˜:
1. Nê°œì˜ "ì…ì"(ê°€ì¤‘ì¹˜)ë¥¼ ë™ì‹œì— ìœ ì§€
2. ê° ì…ìëŠ” ë…ë¦½ì ìœ¼ë¡œ gradient descent
3. ì£¼ê¸°ì ìœ¼ë¡œ ì§‘ë‹¨ ì •ë³´ ê³µìœ  (í‰ê· , ìµœì„ )
4. ì„±ëŠ¥ ë‚˜ìœ ì…ìëŠ” "ëŒì—°ë³€ì´" ë˜ëŠ” "ì¬ìƒì„±"
5. ì–‘ì í„°ë„ë§: í™•ë¥ ì ìœ¼ë¡œ ë‚˜ìœ ë°©í–¥ë„ ì‹œë„
6. ì˜¨ë„ ê°ì†Œ: íƒìƒ‰ â†’ ìˆ˜ë ´
7. ìµœì¢…: ì•™ìƒë¸” ê²°í•©
"""

import numpy as np
import matplotlib.pyplot as plt
import time
from typing import List, Tuple


class LightweightNN:
    """ê²½ëŸ‰ ì‹ ê²½ë§"""

    def __init__(self, seed=None):
        if seed:
            np.random.seed(seed)
        self.W1 = np.random.randn(4, 6) * 0.1
        self.b1 = np.zeros(6)
        self.W2 = np.random.randn(6, 1) * 0.1
        self.b2 = np.zeros(1)

    def forward(self, X):
        self.X = X
        self.z1 = X @ self.W1 + self.b1
        self.a1 = np.maximum(0, self.z1)
        self.z2 = self.a1 @ self.W2 + self.b2
        return self.z2

    def loss(self, X, y):
        pred = self.forward(X)
        return np.mean((pred - y) ** 2)

    def get_weights(self):
        return np.concatenate([
            self.W1.flatten(), self.b1,
            self.W2.flatten(), self.b2
        ])

    def set_weights(self, w):
        self.W1 = w[:24].reshape(4, 6)
        self.b1 = w[24:30]
        self.W2 = w[30:36].reshape(6, 1)
        self.b2 = w[36:37]

    def gradient(self, X, y):
        m = len(X)
        pred = self.forward(X)
        dz2 = 2 * (pred - y) / m
        dW2 = self.a1.T @ dz2
        db2 = np.sum(dz2, axis=0)
        da1 = dz2 @ self.W2.T
        dz1 = da1 * (self.z1 > 0)
        dW1 = X.T @ dz1
        db1 = np.sum(dz1, axis=0)
        return np.concatenate([
            dW1.flatten(), db1,
            dW2.flatten(), db2
        ])

    def copy(self):
        new = LightweightNN()
        new.set_weights(self.get_weights().copy())
        return new


class Particle:
    """
    ì…ì: ê°€ì¤‘ì¹˜ ê³µê°„ì„ íƒìƒ‰í•˜ëŠ” agent

    ê°œë…:
    - ìœ„ì¹˜: í˜„ì¬ ê°€ì¤‘ì¹˜
    - ì†ë„: ì—…ë°ì´íŠ¸ ë°©í–¥ (momentum)
    - ì—ë„ˆì§€: ì†ì‹¤ê°’ (ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ)
    - ì˜¨ë„: íƒìƒ‰ ì •ë„ (ë†’ìœ¼ë©´ ëœë¤ íƒìƒ‰ ê°•í™”)
    """

    def __init__(self, network: LightweightNN, particle_id: int):
        self.network = network
        self.id = particle_id
        self.velocity = np.zeros_like(network.get_weights())
        self.best_position = network.get_weights().copy()
        self.best_loss = float('inf')

    def update(self, X, y, learning_rate: float, temperature: float,
               global_best: np.ndarray, swarm_center: np.ndarray):
        """
        ì…ì ì—…ë°ì´íŠ¸

        ì˜ê°:
        - Gradient descent: ê²½ì‚¬ í•˜ê°•
        - Momentum: ê´€ì„±
        - PSO: ìê¸° ìµœì„  + ì§‘ë‹¨ ìµœì„ 
        - ì–‘ì í„°ë„ë§: í™•ë¥ ì  ì í”„
        """
        # 1. Gradient ê³„ì‚°
        grad = self.network.gradient(X, y)

        # 2. ì—¬ëŸ¬ í˜ì˜ ì¡°í•©
        current_pos = self.network.get_weights()

        # Force 1: Gradient (ê¸°ë³¸ í•™ìŠµ)
        force_gradient = -grad

        # Force 2: Momentum (ê´€ì„±)
        force_momentum = 0.9 * self.velocity

        # Force 3: Attraction to personal best (ìê¸° ìµœì„ )
        force_personal = 0.5 * (self.best_position - current_pos)

        # Force 4: Attraction to global best (ì§‘ë‹¨ ìµœì„ )
        force_global = 0.3 * (global_best - current_pos)

        # Force 5: Attraction to swarm center (ì§‘ë‹¨ ì¤‘ì‹¬)
        force_center = 0.2 * (swarm_center - current_pos)

        # Force 6: Quantum tunneling (ì–‘ì í„°ë„ë§)
        force_quantum = np.random.randn(len(current_pos)) * temperature

        # ì´ í˜
        total_force = (
            force_gradient +
            force_momentum +
            force_personal +
            force_global +
            force_center +
            force_quantum
        )

        # 3. ì†ë„ ì—…ë°ì´íŠ¸
        self.velocity = total_force

        # 4. ìœ„ì¹˜ ì—…ë°ì´íŠ¸
        new_position = current_pos + learning_rate * self.velocity
        self.network.set_weights(new_position)

        # 5. í‰ê°€
        current_loss = self.network.loss(X, y)

        # 6. ê°œì¸ ìµœì„  ì—…ë°ì´íŠ¸
        if current_loss < self.best_loss:
            self.best_loss = current_loss
            self.best_position = new_position.copy()

        return current_loss


class QEDOptimizer:
    """
    QED: Quantum-Inspired Ensemble Descent

    í•µì‹¬:
    - Nê°œì˜ ì…ìê°€ í˜‘ë ¥í•˜ì—¬ ìµœì í•´ íƒìƒ‰
    - ì–‘ì ì¤‘ì²© ëª¨ë°©: ì—¬ëŸ¬ ìƒíƒœ ë™ì‹œ ìœ ì§€
    - ì§„í™”ì  ì„ íƒ: ë‚˜ìœ ì…ìëŠ” ì¬ìƒì„±/ë³€ì´
    - ì˜¨ë„ ì¡°ì ˆ: íƒìƒ‰ â†’ ìˆ˜ë ´
    """

    def __init__(self,
                 network_template: LightweightNN,
                 n_particles: int = 10,
                 learning_rate: float = 0.1,
                 temperature_init: float = 0.5,
                 temperature_decay: float = 0.95):
        self.n_particles = n_particles
        self.lr = learning_rate
        self.temperature = temperature_init
        self.temp_decay = temperature_decay

        # ì…ì ì´ˆê¸°í™”
        self.particles: List[Particle] = []
        for i in range(n_particles):
            net = network_template.copy()
            # ê° ì…ìë¥¼ ì•½ê°„ ë‹¤ë¥´ê²Œ ì´ˆê¸°í™”
            noise = np.random.randn(len(net.get_weights())) * 0.1
            net.set_weights(net.get_weights() + noise)
            particle = Particle(net, i)
            self.particles.append(particle)

        # ì „ì—­ ìµœì„ 
        self.global_best = network_template.get_weights().copy()
        self.global_best_loss = float('inf')

        # ì¶”ì 
        self.history = {
            'loss': [],
            'best_loss': [],
            'diversity': [],
            'temperature': []
        }

    def get_swarm_center(self) -> np.ndarray:
        """ì§‘ë‹¨ì˜ ì¤‘ì‹¬ (í‰ê·  ìœ„ì¹˜)"""
        positions = [p.network.get_weights() for p in self.particles]
        return np.mean(positions, axis=0)

    def get_diversity(self) -> float:
        """ì§‘ë‹¨ì˜ ë‹¤ì–‘ì„± (ë¶„ì‚°)"""
        positions = [p.network.get_weights() for p in self.particles]
        return np.std(positions)

    def evolve(self, X, y):
        """
        ì§„í™”ì  ì„ íƒ: ë‚˜ìœ ì…ìëŠ” ë³€ì´/ì¬ìƒì„±

        ì „ëµ:
        1. í•˜ìœ„ 30% ì…ì ì„ íƒ
        2. Crossover: ì¢‹ì€ ì…ì 2ê°œ ê²°í•©
        3. Mutation: ëœë¤ ë³€ì´
        """
        # ì„±ëŠ¥ìœ¼ë¡œ ì •ë ¬
        losses = [(p.network.loss(X, y), i) for i, p in enumerate(self.particles)]
        losses.sort()

        # í•˜ìœ„ 30%
        n_replace = max(1, self.n_particles // 3)
        worst_indices = [idx for _, idx in losses[-n_replace:]]
        best_indices = [idx for _, idx in losses[:3]]

        for i in worst_indices:
            # ì „ëµ 1: Crossover (80% í™•ë¥ )
            if np.random.rand() < 0.8:
                # ì¢‹ì€ ì…ì 2ê°œ ì„ íƒ
                parent1 = self.particles[np.random.choice(best_indices)]
                parent2 = self.particles[np.random.choice(best_indices)]

                # ê²°í•© (í‰ê·  + ì•½ê°„ì˜ ë³€ì´)
                w1 = parent1.network.get_weights()
                w2 = parent2.network.get_weights()
                child_w = 0.5 * (w1 + w2) + np.random.randn(len(w1)) * 0.1

                self.particles[i].network.set_weights(child_w)
                self.particles[i].velocity *= 0  # ì†ë„ ë¦¬ì…‹

            # ì „ëµ 2: Mutation (20% í™•ë¥ )
            else:
                # ì „ì—­ ìµœì„ ì—ì„œ ì¶œë°œ + í° ë³€ì´
                mutated = self.global_best + np.random.randn(len(self.global_best)) * 0.3
                self.particles[i].network.set_weights(mutated)
                self.particles[i].velocity *= 0

    def train(self, X, y, max_iters: int = 100, verbose: bool = True):
        """QED í•™ìŠµ"""
        start = time.time()

        for iteration in range(max_iters):
            swarm_center = self.get_swarm_center()
            losses = []

            # ê° ì…ì ì—…ë°ì´íŠ¸
            for particle in self.particles:
                loss = particle.update(
                    X, y,
                    self.lr,
                    self.temperature,
                    self.global_best,
                    swarm_center
                )
                losses.append(loss)

                # ì „ì—­ ìµœì„  ì—…ë°ì´íŠ¸
                if loss < self.global_best_loss:
                    self.global_best_loss = loss
                    self.global_best = particle.network.get_weights().copy()

            # ì§„í™” (ë§¤ 5 iterationë§ˆë‹¤)
            if iteration % 5 == 0:
                self.evolve(X, y)

            # ì˜¨ë„ ê°ì†Œ (íƒìƒ‰ â†’ ìˆ˜ë ´)
            self.temperature *= self.temp_decay

            # ì¶”ì 
            avg_loss = np.mean(losses)
            diversity = self.get_diversity()
            self.history['loss'].append(avg_loss)
            self.history['best_loss'].append(self.global_best_loss)
            self.history['diversity'].append(diversity)
            self.history['temperature'].append(self.temperature)

            if verbose and iteration % 10 == 0:
                print(f"[{iteration:3d}] "
                      f"Avg Loss: {avg_loss:.5f} | "
                      f"Best: {self.global_best_loss:.5f} | "
                      f"Diversity: {diversity:.3f} | "
                      f"Temp: {self.temperature:.3f}")

            # ì¡°ê¸° ì¢…ë£Œ
            if self.global_best_loss < 0.01:
                if verbose:
                    print(f"\nâœ“ Converged at iteration {iteration}")
                break

        elapsed = time.time() - start

        # ìµœì¢…: ì•™ìƒë¸” ê²°í•© (ìƒìœ„ 3ê°œ í‰ê· )
        top_particles = sorted(self.particles,
                              key=lambda p: p.best_loss)[:3]
        ensemble_weights = np.mean([p.best_position for p in top_particles], axis=0)

        return {
            'final_loss': self.global_best_loss,
            'iterations': len(self.history['loss']),
            'time': elapsed,
            'best_weights': ensemble_weights
        }


class SGDOptimizer:
    """í‘œì¤€ SGD (ë¹„êµìš©)"""

    def __init__(self, network, learning_rate=0.1):
        self.net = network
        self.lr = learning_rate
        self.history = {'loss': []}

    def train(self, X, y, max_iters=100, verbose=False):
        start = time.time()
        for it in range(max_iters):
            grad = self.net.gradient(X, y)
            w = self.net.get_weights()
            w -= self.lr * grad
            self.net.set_weights(w)
            loss = self.net.loss(X, y)
            self.history['loss'].append(loss)
            if verbose and it % 10 == 0:
                print(f"[{it:3d}] Loss: {loss:.5f}")
            if loss < 0.01:
                break
        elapsed = time.time() - start
        return {
            'final_loss': self.history['loss'][-1],
            'iterations': len(self.history['loss']),
            'time': elapsed
        }


def make_dataset(name='nonlinear', n=100):
    """ë°ì´í„°ì…‹ ìƒì„±"""
    np.random.seed(42)
    X = np.random.randn(n, 4)
    if name == 'linear':
        y = (X @ [1, -0.5, 0.3, 0.8]).reshape(-1, 1)
    elif name == 'nonlinear':
        y = (np.sin(X[:, 0]) + np.cos(X[:, 1]) * X[:, 2]).reshape(-1, 1)
    elif name == 'xor':
        y = ((X[:, 0] > 0) ^ (X[:, 1] > 0)).astype(float).reshape(-1, 1)
    X = (X - X.mean(0)) / (X.std(0) + 1e-8)
    y = (y - y.mean()) / (y.std() + 1e-8)
    return X, y


def run_qed_experiment(dataset_name='nonlinear'):
    """QED ì‹¤í—˜"""
    print(f"\n{'='*80}")
    print(f"ğŸš€ QED vs SGD: {dataset_name.upper()}")
    print(f"{'='*80}\n")

    X, y = make_dataset(dataset_name, n=100)

    # QED
    print("1ï¸âƒ£  QED (Quantum-Inspired Ensemble Descent)")
    print("-" * 80)
    net_qed = LightweightNN(seed=42)
    opt_qed = QEDOptimizer(
        net_qed,
        n_particles=10,
        learning_rate=0.05,
        temperature_init=0.3,
        temperature_decay=0.97
    )
    result_qed = opt_qed.train(X, y, max_iters=100, verbose=True)

    # SGD
    print(f"\n2ï¸âƒ£  Standard SGD")
    print("-" * 80)
    net_sgd = LightweightNN(seed=42)
    opt_sgd = SGDOptimizer(net_sgd, learning_rate=0.1)
    result_sgd = opt_sgd.train(X, y, max_iters=100, verbose=True)

    # ë¹„êµ
    print(f"\n{'='*80}")
    print("ğŸ“Š ê²°ê³¼")
    print(f"{'='*80}")
    print(f"{'ì§€í‘œ':<30} {'QED':>25} {'SGD':>20}")
    print("-" * 80)
    print(f"{'ìµœì¢… ì†ì‹¤':<30} {result_qed['final_loss']:>25.6f} {result_sgd['final_loss']:>20.6f}")
    print(f"{'ìˆ˜ë ´ ë°˜ë³µ íšŸìˆ˜':<30} {result_qed['iterations']:>25d} {result_sgd['iterations']:>20d}")
    print(f"{'ì‹œê°„ (ì´ˆ)':<30} {result_qed['time']:>25.4f} {result_sgd['time']:>20.4f}")

    improvement = (result_sgd['final_loss'] - result_qed['final_loss']) / result_sgd['final_loss'] * 100

    if improvement > 0:
        print(f"{'ğŸ¯ QED ê°œì„ ìœ¨':<30} {improvement:>25.2f}%")
        print("="*80)
        print("âœ… QED ìŠ¹ë¦¬!")
    else:
        print(f"{'âŒ QED ì•…í™”':<30} {improvement:>25.2f}%")
        print("="*80)
        print("SGD ìŠ¹ë¦¬")

    return {
        'qed': result_qed,
        'sgd': result_sgd,
        'qed_opt': opt_qed,
        'sgd_opt': opt_sgd,
        'dataset': dataset_name
    }


def plot_qed(results):
    """ì‹œê°í™”"""
    qed = results['qed_opt']
    sgd = results['sgd_opt']
    ds = results['dataset']

    fig, axes = plt.subplots(2, 2, figsize=(16, 10))
    fig.suptitle(f'QED vs SGD: {ds.upper()}', fontsize=18, fontweight='bold')

    # 1. Loss ë¹„êµ
    ax = axes[0, 0]
    ax.plot(qed.history['best_loss'], 'b-', label='QED (Best)', linewidth=2.5)
    ax.plot(qed.history['loss'], 'b--', label='QED (Avg)', linewidth=1.5, alpha=0.6)
    ax.plot(sgd.history['loss'], 'r-', label='SGD', linewidth=2.5)
    ax.set_xlabel('Iteration', fontsize=12)
    ax.set_ylabel('Loss', fontsize=12)
    ax.set_title('Learning Curves', fontsize=14, fontweight='bold')
    ax.legend(fontsize=11)
    ax.grid(True, alpha=0.3)
    ax.set_yscale('log')

    # 2. Diversity
    ax = axes[0, 1]
    ax.plot(qed.history['diversity'], 'g-', linewidth=2.5)
    ax.set_xlabel('Iteration', fontsize=12)
    ax.set_ylabel('Diversity (Std)', fontsize=12)
    ax.set_title('Swarm Diversity', fontsize=14, fontweight='bold')
    ax.grid(True, alpha=0.3)

    # 3. Temperature
    ax = axes[1, 0]
    ax.plot(qed.history['temperature'], 'm-', linewidth=2.5)
    ax.set_xlabel('Iteration', fontsize=12)
    ax.set_ylabel('Temperature', fontsize=12)
    ax.set_title('Exploration Temperature', fontsize=14, fontweight='bold')
    ax.grid(True, alpha=0.3)

    # 4. ìµœì¢… ë¹„êµ
    ax = axes[1, 1]
    metrics = ['Final Loss', 'Iterations']
    qed_vals = [results['qed']['final_loss'], results['qed']['iterations']]
    sgd_vals = [results['sgd']['final_loss'], results['sgd']['iterations']]

    x = np.arange(len(metrics))
    width = 0.35

    # Normalize for visualization
    qed_norm = [qed_vals[0]*100, qed_vals[1]]
    sgd_norm = [sgd_vals[0]*100, sgd_vals[1]]

    bars1 = ax.bar(x - width/2, qed_norm, width, label='QED', alpha=0.8, color='blue')
    bars2 = ax.bar(x + width/2, sgd_norm, width, label='SGD', alpha=0.8, color='red')

    ax.set_ylabel('Value', fontsize=12)
    ax.set_title('Performance Comparison', fontsize=14, fontweight='bold')
    ax.set_xticks(x)
    ax.set_xticklabels(metrics)
    ax.legend(fontsize=11)
    ax.grid(True, alpha=0.3, axis='y')

    plt.tight_layout()
    filename = f'/Users/say/Documents/GitHub/ai/qed_{ds}_results.png'
    plt.savefig(filename, dpi=300, bbox_inches='tight')
    print(f"\nâœ… ì €ì¥: {filename}")


if __name__ == "__main__":
    print("\n" + "="*80)
    print("ğŸš€ QED: Quantum-Inspired Ensemble Descent")
    print("   ì™„ì „íˆ ìƒˆë¡œìš´ ìµœì í™” íŒ¨ëŸ¬ë‹¤ì„")
    print("="*80)

    datasets = ['linear', 'nonlinear', 'xor']
    all_results = {}
    wins = 0
    total = len(datasets)

    for ds in datasets:
        result = run_qed_experiment(ds)
        all_results[ds] = result
        plot_qed(result)

        if result['qed']['final_loss'] < result['sgd']['final_loss']:
            wins += 1

    print(f"\n{'='*80}")
    print("ğŸ¯ ìµœì¢… ìš”ì•½")
    print(f"{'='*80}")

    for ds, res in all_results.items():
        qed_loss = res['qed']['final_loss']
        sgd_loss = res['sgd']['final_loss']
        improvement = (sgd_loss - qed_loss) / sgd_loss * 100

        if improvement > 0:
            status = "âœ… QED ìŠ¹ë¦¬"
        else:
            status = "âŒ SGD ìŠ¹ë¦¬"

        print(f"{ds.upper():12s} | {status:15s} | {improvement:+7.2f}%")

    print(f"\n{'='*80}")
    print(f"QED ìŠ¹ë¥ : {wins}/{total} ({wins/total*100:.1f}%)")
    print(f"{'='*80}\n")
