"""
GENESIS Paradigm Comparison v2 (Ultrathink Edition)
Author: GENESIS Project
Date: 2026-01-03

í•µì‹¬ ê°œì„ ì‚¬í•­:
    1. TRUE Viability Entity ì¶”ê°€ (vs v1.1 Hebbian)
    2. ê°€í˜¹í•œ í™˜ê²½ (ë†’ì€ ì†Œëª¨, ë‚®ì€ ë³´ìƒ)
    3. ì§„ì •í•œ ìƒì¡´ ì••ë ¥ (ì£½ìŒ ê°€ëŠ¥)
    4. ì˜ë¯¸ìˆëŠ” ì°¨ë³„í™”

ë¹„êµ ëŒ€ìƒ:
    1. True Viability (GENESIS) - ì˜ˆì¸¡ + í•­ìƒì„± + ì•Œë¡œìŠ¤íƒ€ì‹œìŠ¤
    2. Pure Viability (v1.1) - Hebbian + íƒìƒ‰
    3. Supervised Learning - Gradient descent
    4. Reinforcement Learning - Policy gradient
    5. Random Baseline - No learning

ì˜ˆìƒ ê²°ê³¼:
    - True Viabilityê°€ ë” ì˜¤ë˜ ìƒì¡´ (ì˜ˆì¸¡ ëŠ¥ë ¥)
    - Pure ViabilityëŠ” ì ì‘ì ì´ì§€ë§Œ ì˜ˆì¸¡ ì—†ìŒ
    - SupervisedëŠ” ground truth ì˜ì¡´
    - RLì€ ìƒ˜í”Œ íš¨ìœ¨ ë‚®ìŒ
    - Randomì€ ë¹ ë¥´ê²Œ ì£½ìŒ
"""

import sys
import os
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', 'core'))

import numpy as np
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
from collections import defaultdict

from true_viability_entity import TrueViabilityEntity
from pure_viability_entity import PureViabilityEntity
from pure_viability_environment import ResourceEnvironment


class SupervisedEntity:
    """Supervised Learning (í‘œì¤€ ML)"""

    def __init__(self, state_size=5, action_size=1, hidden_size=16, initial_energy=5.0):
        self.state_size = state_size
        self.action_size = action_size
        self.is_alive = True
        self.energy = initial_energy
        self.age = 0

        # ë„¤íŠ¸ì›Œí¬
        self.W1 = np.random.randn(state_size, hidden_size) * 0.1
        self.W2 = np.random.randn(hidden_size, action_size) * 0.1

        self.energy_history = []

    def forward(self, state):
        hidden = np.tanh(np.dot(state, self.W1))
        action = np.tanh(np.dot(hidden, self.W2))
        return action

    def live_one_step(self, environment):
        if not self.is_alive:
            return {'is_alive': False}

        self.age += 1
        state = environment.get_state()
        action = self.forward(state)

        next_state, energy_change, done, info = environment.step(action.flatten()[0])

        # Supervised learning: ì •ë‹µ ì‚¬ìš©!
        target = info['optimal_action']
        error = action.flatten()[0] - target

        # Gradient descent
        learning_rate = 0.01
        grad_action = error
        grad_W2 = np.outer(np.tanh(np.dot(state, self.W1)), grad_action)
        self.W2 -= learning_rate * grad_W2

        # ì—ë„ˆì§€ ì—…ë°ì´íŠ¸
        self.energy += energy_change
        self.energy_history.append(self.energy)

        if self.energy <= 0:
            self.is_alive = False

        return {
            'is_alive': self.is_alive,
            'age': self.age,
            'energy': self.energy,
            'energy_change': energy_change
        }


class RLEntity:
    """Reinforcement Learning (REINFORCE)"""

    def __init__(self, state_size=5, action_size=1, hidden_size=16, initial_energy=5.0):
        self.state_size = state_size
        self.action_size = action_size
        self.is_alive = True
        self.energy = initial_energy
        self.age = 0

        # Policy network
        self.W1 = np.random.randn(state_size, hidden_size) * 0.1
        self.W2 = np.random.randn(hidden_size, action_size) * 0.1

        # Episode memory
        self.episode_states = []
        self.episode_actions = []
        self.episode_rewards = []

        self.energy_history = []

    def forward(self, state):
        hidden = np.tanh(np.dot(state, self.W1))
        action_mean = np.tanh(np.dot(hidden, self.W2))
        # Stochastic policy
        action = action_mean + np.random.randn(*action_mean.shape) * 0.1
        return action

    def live_one_step(self, environment):
        if not self.is_alive:
            return {'is_alive': False}

        self.age += 1
        state = environment.get_state()
        action = self.forward(state)

        next_state, energy_change, done, info = environment.step(action.flatten()[0])

        # Store transition
        self.episode_states.append(state)
        self.episode_actions.append(action)
        self.episode_rewards.append(energy_change)

        # Policy gradient update (every 10 steps)
        if len(self.episode_rewards) >= 10:
            self._policy_gradient_update()
            self.episode_states = []
            self.episode_actions = []
            self.episode_rewards = []

        # ì—ë„ˆì§€ ì—…ë°ì´íŠ¸
        self.energy += energy_change
        self.energy_history.append(self.energy)

        if self.energy <= 0:
            self.is_alive = False

        return {
            'is_alive': self.is_alive,
            'age': self.age,
            'energy': self.energy,
            'energy_change': energy_change
        }

    def _policy_gradient_update(self):
        """REINFORCE algorithm"""
        if len(self.episode_rewards) == 0:
            return

        # Compute returns
        returns = []
        G = 0
        for r in reversed(self.episode_rewards):
            G = r + 0.99 * G
            returns.insert(0, G)

        returns = np.array(returns)
        returns = (returns - np.mean(returns)) / (np.std(returns) + 1e-8)

        # Policy gradient
        learning_rate = 0.001
        for state, action, G in zip(self.episode_states, self.episode_actions, returns):
            hidden = np.tanh(np.dot(state, self.W1))
            grad_W2 = np.outer(hidden, action) * G
            self.W2 += learning_rate * grad_W2


class RandomEntity:
    """Random Baseline (no learning)"""

    def __init__(self, state_size=5, action_size=1, hidden_size=16, initial_energy=5.0):
        self.is_alive = True
        self.energy = initial_energy
        self.age = 0
        self.energy_history = []

    def live_one_step(self, environment):
        if not self.is_alive:
            return {'is_alive': False}

        self.age += 1
        state = environment.get_state()

        # Random action
        action = np.random.randn() * 2.0

        next_state, energy_change, done, info = environment.step(action)

        # ì—ë„ˆì§€ ì—…ë°ì´íŠ¸
        self.energy += energy_change
        self.energy_history.append(self.energy)

        if self.energy <= 0:
            self.is_alive = False

        return {
            'is_alive': self.is_alive,
            'age': self.age,
            'energy': self.energy,
            'energy_change': energy_change
        }


def run_single_trial(paradigm_name, entity, env, n_steps=500):
    """ë‹¨ì¼ ì‹œí–‰ ì‹¤í–‰"""
    env.reset()

    results = {
        'survival_steps': 0,
        'final_energy': 0.0,
        'energy_history': [],
        'died': False
    }

    for step in range(n_steps):
        result = entity.live_one_step(env)

        if not result['is_alive']:
            results['survival_steps'] = result['age']
            results['final_energy'] = result['energy']
            results['died'] = True
            break

        results['energy_history'].append(result['energy'])
        results['survival_steps'] = result['age']
        results['final_energy'] = result['energy']

    return results


def run_paradigm_comparison(n_trials=10, n_steps=500, harsh_mode=True):
    """
    ì „ì²´ íŒ¨ëŸ¬ë‹¤ì„ ë¹„êµ ì‹¤í—˜

    Args:
        n_trials: ì‹œí–‰ íšŸìˆ˜
        n_steps: ìµœëŒ€ ìŠ¤í… ìˆ˜
        harsh_mode: Trueë©´ ê°€í˜¹í•œ í™˜ê²½
    """
    print("=" * 70)
    print("PARADIGM COMPARISON v2 (ULTRATHINK EDITION)")
    print("=" * 70)

    # í™˜ê²½ ì„¤ì •
    if harsh_mode:
        print("\nğŸ”¥ HARSH ENVIRONMENT MODE ğŸ”¥")
        print("  Energy cost: 0.2 per step (high)")
        print("  Reward scale: 0.3 (low)")
        print("  Function: nonlinear (complex)")
        print("  Initial energy: 5.0 (limited)")
        env_kwargs = {
            'input_dim': 5,
            'function_type': 'nonlinear',
            'energy_reward_scale': 0.3,
            'energy_cost_per_step': 0.2
        }
    else:
        print("\nStandard Environment")
        env_kwargs = {
            'input_dim': 5,
            'function_type': 'linear',
            'energy_reward_scale': 1.0,
            'energy_cost_per_step': 0.05
        }

    print(f"\nTrials: {n_trials}")
    print(f"Max steps: {n_steps}")

    # íŒ¨ëŸ¬ë‹¤ì„ ì •ì˜
    paradigms = [
        ('True Viability (GENESIS v2)', lambda: TrueViabilityEntity(
            state_size=5, action_size=1, hidden_size=32, initial_energy=5.0
        )),
        ('Pure Viability (v1.1)', lambda: PureViabilityEntity(
            input_size=5, hidden_size=16, output_size=1, initial_energy=5.0
        )),
        ('Supervised Learning (SGD)', lambda: SupervisedEntity(
            state_size=5, action_size=1, hidden_size=16, initial_energy=5.0
        )),
        ('Reinforcement Learning', lambda: RLEntity(
            state_size=5, action_size=1, hidden_size=16, initial_energy=5.0
        )),
        ('Random Baseline', lambda: RandomEntity(
            state_size=5, action_size=1, hidden_size=16, initial_energy=5.0
        ))
    ]

    # ê²°ê³¼ ì €ì¥
    all_results = defaultdict(list)

    # ê° íŒ¨ëŸ¬ë‹¤ì„ ì‹¤í—˜
    for paradigm_name, entity_factory in paradigms:
        print(f"\n{'='*70}")
        print(f"Testing: {paradigm_name}")
        print(f"{'='*70}")

        for trial in range(n_trials):
            # ìƒˆ í™˜ê²½ê³¼ entity ìƒì„±
            env = ResourceEnvironment(**env_kwargs, seed=42 + trial)
            entity = entity_factory()

            # ì‹œí–‰ ì‹¤í–‰
            result = run_single_trial(paradigm_name, entity, env, n_steps)

            all_results[paradigm_name].append(result)

            # ì§„í–‰ ìƒí™© ì¶œë ¥
            if result['died']:
                print(f"  Trial {trial+1}/{n_trials}: ğŸ’€ Died at step {result['survival_steps']} "
                      f"(energy: {result['final_energy']:.2f})")
            else:
                print(f"  Trial {trial+1}/{n_trials}: âœ“ Survived {result['survival_steps']} steps "
                      f"(energy: {result['final_energy']:.2f})")

    # ê²°ê³¼ ë¶„ì„
    print(f"\n{'='*70}")
    print("COMPARATIVE ANALYSIS")
    print(f"{'='*70}\n")

    # í—¤ë”
    print(f"{'Paradigm':<35} | {'Avg Survival':>15} | {'Avg Final Energy':>18} | {'Death Rate':>12}")
    print("-" * 95)

    summary_stats = {}

    for paradigm_name in [p[0] for p in paradigms]:
        results = all_results[paradigm_name]

        survival_steps = [r['survival_steps'] for r in results]
        final_energies = [r['final_energy'] for r in results]
        death_count = sum([r['died'] for r in results])
        death_rate = death_count / len(results) * 100

        avg_survival = np.mean(survival_steps)
        std_survival = np.std(survival_steps)
        avg_energy = np.mean(final_energies)
        std_energy = np.std(final_energies)

        summary_stats[paradigm_name] = {
            'avg_survival': avg_survival,
            'std_survival': std_survival,
            'avg_energy': avg_energy,
            'std_energy': std_energy,
            'death_rate': death_rate,
            'all_results': results
        }

        print(f"{paradigm_name:<35} | {avg_survival:7.1f} Â± {std_survival:5.1f} | "
              f"{avg_energy:8.2f} Â± {std_energy:6.2f} | {death_rate:10.1f}%")

    return summary_stats


def plot_comparison(summary_stats, save_path='../../results/paradigm_comparison_v2.png'):
    """ë¹„êµ ê²°ê³¼ ì‹œê°í™”"""

    fig, axes = plt.subplots(2, 2, figsize=(14, 10))

    paradigm_names = list(summary_stats.keys())
    colors = ['#2E86AB', '#A23B72', '#F18F01', '#C73E1D', '#6C757D']

    # Plot 1: Survival Steps
    ax = axes[0, 0]
    avg_survivals = [summary_stats[p]['avg_survival'] for p in paradigm_names]
    std_survivals = [summary_stats[p]['std_survival'] for p in paradigm_names]

    bars = ax.bar(range(len(paradigm_names)), avg_survivals,
                   yerr=std_survivals, capsize=5, color=colors, alpha=0.7)
    ax.set_xticks(range(len(paradigm_names)))
    ax.set_xticklabels([p.split('(')[0].strip() for p in paradigm_names],
                        rotation=15, ha='right', fontsize=9)
    ax.set_ylabel('Average Survival Steps', fontsize=11)
    ax.set_title('Survival Capacity', fontsize=12, fontweight='bold')
    ax.grid(axis='y', alpha=0.3)

    # Plot 2: Final Energy
    ax = axes[0, 1]
    avg_energies = [summary_stats[p]['avg_energy'] for p in paradigm_names]
    std_energies = [summary_stats[p]['std_energy'] for p in paradigm_names]

    bars = ax.bar(range(len(paradigm_names)), avg_energies,
                   yerr=std_energies, capsize=5, color=colors, alpha=0.7)
    ax.set_xticks(range(len(paradigm_names)))
    ax.set_xticklabels([p.split('(')[0].strip() for p in paradigm_names],
                        rotation=15, ha='right', fontsize=9)
    ax.set_ylabel('Average Final Energy', fontsize=11)
    ax.set_title('Energy Efficiency', fontsize=12, fontweight='bold')
    ax.grid(axis='y', alpha=0.3)
    ax.axhline(y=5.0, color='red', linestyle='--', alpha=0.5, label='Initial Energy')
    ax.legend(fontsize=9)

    # Plot 3: Death Rate
    ax = axes[1, 0]
    death_rates = [summary_stats[p]['death_rate'] for p in paradigm_names]

    bars = ax.bar(range(len(paradigm_names)), death_rates, color=colors, alpha=0.7)
    ax.set_xticks(range(len(paradigm_names)))
    ax.set_xticklabels([p.split('(')[0].strip() for p in paradigm_names],
                        rotation=15, ha='right', fontsize=9)
    ax.set_ylabel('Death Rate (%)', fontsize=11)
    ax.set_title('Mortality Risk', fontsize=12, fontweight='bold')
    ax.set_ylim([0, 100])
    ax.grid(axis='y', alpha=0.3)

    # Plot 4: Energy Trajectories (sample from first trial)
    ax = axes[1, 1]
    for i, paradigm_name in enumerate(paradigm_names):
        first_trial = summary_stats[paradigm_name]['all_results'][0]
        if len(first_trial['energy_history']) > 0:
            ax.plot(first_trial['energy_history'], label=paradigm_name.split('(')[0].strip(),
                   color=colors[i], alpha=0.8, linewidth=2)

    ax.set_xlabel('Time Steps', fontsize=11)
    ax.set_ylabel('Energy Level', fontsize=11)
    ax.set_title('Energy Trajectories (Sample Trial)', fontsize=12, fontweight='bold')
    ax.legend(fontsize=8, loc='best')
    ax.grid(alpha=0.3)
    ax.axhline(y=0, color='red', linestyle='--', alpha=0.5)

    plt.tight_layout()

    # ì €ì¥
    os.makedirs(os.path.dirname(save_path), exist_ok=True)
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    print(f"\nFigure saved: {save_path}")
    plt.close()


if __name__ == "__main__":
    print("""
    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
    â•‘                                                                   â•‘
    â•‘         PARADIGM COMPARISON v2 (ULTRATHINK EDITION)              â•‘
    â•‘                                                                   â•‘
    â•‘  True Viability vs Pure Viability vs Supervised vs RL vs Random  â•‘
    â•‘  Testing under HARSH environmental conditions                    â•‘
    â•‘                                                                   â•‘
    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)

    # ê°€í˜¹í•œ í™˜ê²½ì—ì„œ ì‹¤í—˜
    summary_stats = run_paradigm_comparison(n_trials=10, n_steps=500, harsh_mode=True)

    # ê²°ê³¼ ì‹œê°í™”
    plot_comparison(summary_stats)

    print("\n" + "=" * 70)
    print("EXPERIMENT COMPLETE!")
    print("=" * 70)

    # í•µì‹¬ ë°œê²¬ ì¶œë ¥
    print("\nğŸ” KEY FINDINGS:")
    paradigm_names = list(summary_stats.keys())

    # ìµœê³  ìƒì¡´
    best_survival = max(paradigm_names,
                       key=lambda p: summary_stats[p]['avg_survival'])
    print(f"\nâœ“ Best Survival: {best_survival}")
    print(f"  Avg steps: {summary_stats[best_survival]['avg_survival']:.1f}")

    # ìµœì € ì‚¬ë§ë¥ 
    best_survival_rate = min(paradigm_names,
                            key=lambda p: summary_stats[p]['death_rate'])
    print(f"\nâœ“ Lowest Mortality: {best_survival_rate}")
    print(f"  Death rate: {summary_stats[best_survival_rate]['death_rate']:.1f}%")

    # ìµœê³  ì—ë„ˆì§€
    best_energy = max(paradigm_names,
                     key=lambda p: summary_stats[p]['avg_energy'])
    print(f"\nâœ“ Best Energy Management: {best_energy}")
    print(f"  Avg final energy: {summary_stats[best_energy]['avg_energy']:.2f}")
