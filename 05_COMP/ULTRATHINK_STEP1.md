# ULTRATHINK Step 1: 근본 문제 재정의

## 우리가 진짜 풀려는 문제는?

### 표면적 문제
"SGD보다 나은 최적화 알고리즘 만들기"

### 진짜 문제 (더 깊이 파고들기)

**1단계 질문: 왜 SGD를 이기고 싶은가?**
→ SGD가 비효율적이기 때문

**2단계 질문: SGD는 왜 비효율적인가?**
→ 매 스텝마다 지역적 정보(gradient)만 사용
→ 전역적 구조를 모름
→ 과거 경험을 제대로 활용 못함

**3단계 질문: 그렇다면 진짜 문제는?**
→ **"정보 활용의 비효율성"**

---

## 정보의 종류들

현재까지 우리가 사용한 정보:

| 정보 타입 | LAML | QED | LAML-Q |
|----------|------|-----|---------|
| Gradient (지역) | ✓ | ✓ | ✓ |
| Loss (전역) | ✓ | ✓ | ✓ |
| 과거 궤적 | ✓ | ✓ | ✓ |
| 다중 후보 | ✗ | ✓ | ✓ |
| 물리적 원리 | ✓ | ✗ | ✓ |

### 사용하지 않은 정보들

1. **데이터 구조**: X, y의 기하학적 구조
2. **Loss landscape**: 손실 함수의 형태
3. **네트워크 구조**: 어떤 레이어가 중요한가?
4. **학습 단계**: 초기/중기/후기의 특성
5. **불확실성**: 어디가 확실하고 불확실한가?
6. **메타 패턴**: 여러 학습 과정의 공통 패턴

---

## 핵심 인사이트

### 현재 접근법의 한계

**LAML**: 끝점을 "예측"하려 함
→ 문제: 예측 자체가 또 다른 최적화 문제

**QED**: 여러 방향을 "탐색"함
→ 문제: 탐색이 비효율적 (맹목적)

**LAML-Q**: 둘을 결합
→ 문제: 여전히 "예측 + 탐색" 패러다임

### 새로운 질문

**"예측"하거나 "탐색"하는 대신, 무엇을 할 수 있을까?**

가능한 대안:
1. **학습** - 데이터로부터 최적화 전략을 학습
2. **추론** - 현재 상태에서 최선의 행동을 논리적으로 추론
3. **구성** - 작은 단위들을 조립하여 해를 구성
4. **변환** - 문제를 더 쉬운 공간으로 변환
5. **압축** - 중요한 정보만 추출하여 사용

---

## 가장 근본적인 질문

**"최적화는 본질적으로 무엇인가?"**

### 기존 관점들

1. **수학**: 함수의 최솟값 찾기
2. **물리학**: 에너지 최소화 (LAML의 관점)
3. **생물학**: 진화적 적응 (QED의 관점)
4. **정보이론**: 불확실성 감소

### 새로운 관점 제안

**최적화 = 정보 처리 과정**

입력: 데이터 (X, y), 현재 상태 (θ), 역사
처리: ????? (이것을 새롭게 설계해야 함)
출력: 다음 상태 (θ')

**핵심**: "처리" 부분을 어떻게 설계할 것인가?

---

## 메타 인사이트

지금까지 우리는:
- LAML: 물리학적 원리를 적용
- QED: 생물학적 진화를 적용
- LAML-Q: 둘을 결합

**하지만 이것들은 모두 "비유(analogy)"일 뿐이다.**

### 진짜 질문

**"신경망 최적화만의 고유한 원리는 무엇인가?"**

물리학이나 생물학에서 빌려오는 게 아니라,
신경망 학습 자체의 본질에서 나오는 원리.

---

## Step 1 결론

### 재정의된 문제

**기존**: "SGD를 이기는 알고리즘"
**새로운**: "신경망 학습 과정의 정보를 최대한 활용하는 원리 발견"

### 다음 스텝에서 할 것

Step 2에서 탐색할 질문들:

1. 신경망 학습만의 고유한 특성은?
2. 어떤 정보가 가장 가치있는가?
3. 정보를 어떻게 결합할 것인가?
4. 학습 단계별로 전략이 달라져야 하는가?
5. 네트워크 구조를 활용할 수 있는가?

---

**작성**: 2026-01-03 Step 1/6
**다음**: Step 2 - 기존 가정들 파괴하기
