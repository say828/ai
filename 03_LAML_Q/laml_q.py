"""
LAML-Q: Lagrangian Action Meta-Learning with Quantum-inspired Ensemble
=======================================================================

LAML + QED + GENESIS ì™„ì „ í†µí•©

í•µì‹¬ ì² í•™ (LAMLì—ì„œ ê³„ìŠ¹):
1. ëì (ìµœì¢… ê°€ì¤‘ì¹˜)ì„ ì˜ˆì¸¡
2. ì‹œì‘â†’ë ê²½ë¡œë¥¼ ì—­ì‚°
3. ìµœì†Œ ì‘ìš© ì›ë¦¬ë¡œ ê²½ë¡œ ê²€ì¦
4. ë¶ˆë§Œì¡±ì‹œ ë³´ì • ë°˜ë³µ

í•µì‹¬ í•´ê²°ì±… (QED/GENESISì—ì„œ):
1. ë‹¨ì¼ ì˜ˆì¸¡ â†’ Nê°œ ëì  í›„ë³´ ì•™ìƒë¸”
2. ì •í™•í•œ ì˜ˆì¸¡ â†’ í™•ë¥ ì  íƒìƒ‰ + ì ì§„ì  ì •ì œ
3. ì§„í™”ì  ì„ íƒìœ¼ë¡œ ì¢‹ì€ í›„ë³´ ê°•í™”
4. ìê¸°ì¡°ì§í™”ë¡œ Edge of Chaos ìœ ì§€

ìˆ˜í•™ì  ê¸°ë°˜:
- Action S[Î¸] = âˆ«[Â½||Î¸Ì‡||Â² + Î»L(Î¸)] dt
- ìµœì†Œ ì‘ìš© ì›ë¦¬: Î´S = 0
- ì˜¤ì¼ëŸ¬-ë¼ê·¸ë‘ì£¼ ë°©ì •ì‹ìœ¼ë¡œ ìµœì  ê²½ë¡œ ìœ ë„
"""

import numpy as np
import matplotlib.pyplot as plt
import time
from typing import List, Tuple, Dict
from dataclasses import dataclass


# =============================================================================
# 1. ê²½ëŸ‰ ì‹ ê²½ë§
# =============================================================================

class LightweightNN:
    """ê²½ëŸ‰ ì‹ ê²½ë§ (4â†’6â†’1)"""

    def __init__(self, seed=None):
        if seed is not None:
            np.random.seed(seed)
        # ì´ 37ê°œ íŒŒë¼ë¯¸í„°
        self.W1 = np.random.randn(4, 6) * 0.1
        self.b1 = np.zeros(6)
        self.W2 = np.random.randn(6, 1) * 0.1
        self.b2 = np.zeros(1)

    def forward(self, X):
        self.X = X
        self.z1 = X @ self.W1 + self.b1
        self.a1 = np.maximum(0, self.z1)  # ReLU
        self.z2 = self.a1 @ self.W2 + self.b2
        return self.z2

    def loss(self, X, y):
        pred = self.forward(X)
        return np.mean((pred - y) ** 2)

    def get_weights(self):
        return np.concatenate([
            self.W1.flatten(), self.b1,
            self.W2.flatten(), self.b2
        ])

    def set_weights(self, w):
        self.W1 = w[:24].reshape(4, 6)
        self.b1 = w[24:30]
        self.W2 = w[30:36].reshape(6, 1)
        self.b2 = w[36:37]

    def gradient(self, X, y):
        m = len(X)
        pred = self.forward(X)
        dz2 = 2 * (pred - y) / m
        dW2 = self.a1.T @ dz2
        db2 = np.sum(dz2, axis=0)
        da1 = dz2 @ self.W2.T
        dz1 = da1 * (self.z1 > 0)
        dW1 = X.T @ dz1
        db1 = np.sum(dz1, axis=0)
        return np.concatenate([dW1.flatten(), db1, dW2.flatten(), db2])

    def copy(self):
        new = LightweightNN()
        new.set_weights(self.get_weights().copy())
        return new


# =============================================================================
# 2. LAML-Q í•µì‹¬: ëì  í›„ë³´ (Endpoint Candidate)
# =============================================================================

@dataclass
class Trajectory:
    """í•™ìŠµ ê¶¤ì : ì‹œì‘ì ì—ì„œ ëì ê¹Œì§€ì˜ ê²½ë¡œ"""
    start: np.ndarray      # Î¸â‚€
    end: np.ndarray        # Î¸* (ì˜ˆì¸¡ëœ ëì )
    path: List[np.ndarray] # ê²½ë¡œìƒì˜ ì ë“¤
    action: float          # Action S[Î¸]
    final_loss: float      # ëì ì—ì„œì˜ ì†ì‹¤


class EndpointCandidate:
    """
    ëì  í›„ë³´: LAMLì˜ "ë©”íƒ€ ì˜ˆì¸¡" ì—­í• 

    ê¸°ì¡´ LAML: ë°ì´í„° â†’ ë‹¨ì¼ ëì  ì˜ˆì¸¡ (ë„ˆë¬´ ì–´ë ¤ì›€)
    LAML-Q: ì—¬ëŸ¬ í›„ë³´ë¥¼ ë™ì‹œì— íƒìƒ‰í•˜ê³  ì§„í™”ì‹œí‚´
    """

    def __init__(self, network: LightweightNN, candidate_id: int):
        self.network = network
        self.id = candidate_id

        # ëì  ì˜ˆì¸¡ (ì´ˆê¸°ì—ëŠ” í˜„ì¬ ìœ„ì¹˜ + ë…¸ì´ì¦ˆ)
        self.predicted_endpoint = network.get_weights().copy()

        # ê°œì¸ ìµœì„ 
        self.best_endpoint = self.predicted_endpoint.copy()
        self.best_action = float('inf')
        self.best_loss = float('inf')

        # ê¶¤ì  ê¸°ë¡
        self.trajectory: Trajectory = None

        # ë©”íƒ€ ì •ë³´
        self.generation = 0
        self.survival_score = 0.0

        # â­ Step 1: Adaptive Learning Rate
        self.learning_rate = 0.1  # ì´ˆê¸°ê°’
        self.success_count = 0
        self.fail_count = 0

    def predict_endpoint(self, X, y, temperature: float,
                         global_best: np.ndarray = None,
                         use_gradient_hint: bool = True) -> np.ndarray:
        """
        ëì  ì˜ˆì¸¡: LAMLì˜ í•µì‹¬ (ê°œì„ ëœ ë²„ì „)

        ì „ëµ:
        1. Gradient lookahead: ì—¬ëŸ¬ step ì‹œë®¬ë ˆì´ì…˜
        2. ì‹¤ì œë¡œ ê°œì„ ë˜ëŠ” ë°©í–¥ë§Œ ì„ íƒ
        3. ë³´ìˆ˜ì  ì˜ˆì¸¡ (ì‘ì€ step)
        """
        current = self.network.get_weights()
        current_loss = self.network.loss(X, y)

        # 1. â­ Step 2: Multi-scale Gradient lookahead (1, 5, 10 steps)
        scales = [1, 5, 10]
        scale_predictions = []
        scale_losses = []

        for n_steps in scales:
            temp_weights = current.copy()
            for _ in range(n_steps):
                temp_net = self.network.copy()
                temp_net.set_weights(temp_weights)
                grad = temp_net.gradient(X, y)
                temp_weights -= 0.05 * grad

            # ê° ìŠ¤ì¼€ì¼ì˜ ì˜ˆì¸¡ í‰ê°€
            temp_net.set_weights(temp_weights)
            pred_loss = temp_net.loss(X, y)
            scale_predictions.append(temp_weights)
            scale_losses.append(pred_loss)

        # ì†ì‹¤ ê¸°ë°˜ ê°€ì¤‘ í‰ê·  (ì¢‹ì€ ì˜ˆì¸¡ì— ë†’ì€ ê°€ì¤‘ì¹˜)
        weights = np.array(scale_losses)
        weights = 1.0 / (weights + 1e-8)  # ì—­ìˆ˜ (ë‚®ì€ loss = ë†’ì€ weight)
        weights = weights / weights.sum()  # ì •ê·œí™”

        # ê°€ì¤‘ í‰ê· ìœ¼ë¡œ ìµœì¢… ì˜ˆì¸¡
        gradient_hint = sum(w * pred for w, pred in zip(weights, scale_predictions))

        # 2. ì „ì—­ ìµœì„  ë°©í–¥ (ìˆìœ¼ë©´)
        if global_best is not None and np.linalg.norm(global_best - current) > 1e-8:
            direction = global_best - current
            direction = direction / (np.linalg.norm(direction) + 1e-8)
            global_hint = current + 0.2 * direction
        else:
            global_hint = current

        # 3. ê°€ì¤‘ ì¡°í•© (gradientë¥¼ ë” ì‹ ë¢°)
        predicted = 0.7 * gradient_hint + 0.3 * global_hint

        # 4. ì‘ì€ íƒìƒ‰ ë…¸ì´ì¦ˆ
        predicted += np.random.randn(len(current)) * temperature * 0.1

        # 5. ê²€ì¦: ì˜ˆì¸¡ì´ ì‹¤ì œë¡œ ê°œì„ ë˜ëŠ”ì§€ í™•ì¸
        temp_net = self.network.copy()
        temp_net.set_weights(predicted)
        predicted_loss = temp_net.loss(X, y)

        # ë§Œì•½ ì†ì‹¤ì´ ì¦ê°€í•˜ë©´ ë” ë³´ìˆ˜ì ìœ¼ë¡œ
        if predicted_loss > current_loss:
            predicted = current + 0.1 * (predicted - current)

        self.predicted_endpoint = predicted
        return predicted

    def compute_trajectory(self, X, y, n_steps: int = 10) -> Trajectory:
        """
        LAML í•µì‹¬: ì‹œì‘â†’ë ê²½ë¡œ ìƒì„± (BVP)

        Boundary Value Problem:
        - ì‹œì‘: Î¸â‚€ = current weights
        - ë: Î¸* = predicted endpoint
        - ë°©ë²•: Smoothstep ë³´ê°„ + ë¬¼ë¦¬ì  ì œì•½
        """
        start = self.network.get_weights()
        end = self.predicted_endpoint

        # ê²½ë¡œ ìƒì„± (Smoothstep ë³´ê°„)
        path = []
        for i in range(n_steps + 1):
            t = i / n_steps
            # Smoothstep: 3tÂ² - 2tÂ³ (ë¬¼ë¦¬ì ìœ¼ë¡œ ìì—°ìŠ¤ëŸ¬ìš´ ê¶¤ì )
            s = t * t * (3 - 2 * t)
            point = start * (1 - s) + end * s
            path.append(point)

        # Action ê³„ì‚°
        action = self.compute_action(path, X, y)

        # ëì  ì†ì‹¤
        temp_net = self.network.copy()
        temp_net.set_weights(end)
        final_loss = temp_net.loss(X, y)

        self.trajectory = Trajectory(
            start=start,
            end=end,
            path=path,
            action=action,
            final_loss=final_loss
        )

        return self.trajectory

    def compute_action(self, path: List[np.ndarray], X, y,
                       lambda_potential: float = 1.0) -> float:
        """
        LAML í•µì‹¬: ìµœì†Œ ì‘ìš© ì›ë¦¬

        Action S[Î¸] = âˆ«[T - V] dt
        where:
            T = Â½||Î¸Ì‡||Â² (ìš´ë™ ì—ë„ˆì§€ = ë³€í™”ëŸ‰)
            V = -L(Î¸)   (í¬í…ì…œ = -ì†ì‹¤, ë‚®ì€ ì†ì‹¤ì´ ë‚®ì€ í¬í…ì…œ)

        ë¬¼ë¦¬ì  í•´ì„:
        - ì‘ì€ Action = íš¨ìœ¨ì ì¸ ê²½ë¡œ
        - ìµœì†Œ Action = ìì—°ì´ ì„ íƒí•˜ëŠ” ê²½ë¡œ
        """
        total_action = 0.0
        temp_net = self.network.copy()

        for i in range(len(path) - 1):
            # ì†ë„ (ë³€í™”ëŸ‰)
            velocity = path[i + 1] - path[i]
            kinetic = 0.5 * np.sum(velocity ** 2)

            # í¬í…ì…œ (ì†ì‹¤)
            temp_net.set_weights(path[i])
            loss = temp_net.loss(X, y)
            potential = lambda_potential * loss

            # ë¼ê·¸ë‘ì§€ì•ˆ L = T + V (ìµœì í™”ì—ì„œëŠ” ë‘˜ ë‹¤ ìµœì†Œí™”)
            lagrangian = kinetic + potential

            # Action ëˆ„ì  (dt = 1)
            total_action += lagrangian

        return total_action

    def update_best(self):
        """ê°œì¸ ìµœì„  ì—…ë°ì´íŠ¸ + Adaptive Learning Rate ì¡°ì •"""
        if self.trajectory is not None:
            if self.trajectory.action < self.best_action:
                self.best_action = self.trajectory.action
                self.best_endpoint = self.trajectory.end.copy()
                self.best_loss = self.trajectory.final_loss
                self.survival_score += 1.0

                # â­ ì„±ê³µ â†’ Learning Rate ì¦ê°€
                self.success_count += 1
                self.fail_count = 0
                if self.success_count >= 3:
                    self.learning_rate = min(0.5, self.learning_rate * 1.2)
                    self.success_count = 0

                return True
            else:
                # ì‹¤íŒ¨ â†’ Learning Rate ê°ì†Œ
                self.fail_count += 1
                self.success_count = 0
                if self.fail_count >= 3:
                    self.learning_rate = max(0.01, self.learning_rate * 0.8)
                    self.fail_count = 0

        return False


# =============================================================================
# 3. LAML-Q ì˜µí‹°ë§ˆì´ì €
# =============================================================================

class LAML_Q_Optimizer:
    """
    LAML-Q: Lagrangian Action Meta-Learning with Quantum-inspired Ensemble

    ì™„ì „í•œ í†µí•©:
    - LAML: ëì  ì˜ˆì¸¡ â†’ ê²½ë¡œ ì—­ì‚° â†’ ìµœì†Œ ì‘ìš© ê²€ì¦ â†’ ë³´ì •
    - QED: ì•™ìƒë¸” íƒìƒ‰, ì–‘ì í„°ë„ë§, ì§„í™”ì  ì„ íƒ
    - GENESIS: ìê¸°ì¡°ì§í™”, ì„ê³„ì„±, ì—´ì—­í•™ì  íš¨ìœ¨ì„±
    """

    def __init__(self,
                 network_template: LightweightNN,
                 n_candidates: int = 10,
                 temperature_init: float = 0.5,
                 temperature_decay: float = 0.95,
                 action_threshold: float = 0.1):

        self.n_candidates = n_candidates
        self.temperature = temperature_init
        self.temp_decay = temperature_decay
        self.action_threshold = action_threshold

        # ëì  í›„ë³´ë“¤ ì´ˆê¸°í™”
        self.candidates: List[EndpointCandidate] = []
        for i in range(n_candidates):
            net = network_template.copy()
            # ê° í›„ë³´ë¥¼ ì•½ê°„ ë‹¤ë¥´ê²Œ ì´ˆê¸°í™”
            noise = np.random.randn(len(net.get_weights())) * 0.1
            net.set_weights(net.get_weights() + noise)
            candidate = EndpointCandidate(net, i)
            self.candidates.append(candidate)

        # ì „ì—­ ìµœì„ 
        self.global_best_endpoint = network_template.get_weights().copy()
        self.global_best_action = float('inf')
        self.global_best_loss = float('inf')

        # íˆìŠ¤í† ë¦¬
        self.history = {
            'loss': [],
            'best_loss': [],
            'action': [],
            'best_action': [],
            'diversity': [],
            'temperature': [],
            'acceptance_rate': []
        }

    def get_diversity(self) -> float:
        """í›„ë³´ë“¤ì˜ ë‹¤ì–‘ì„± (ëì  ë¶„ì‚°)"""
        endpoints = [c.predicted_endpoint for c in self.candidates]
        return np.std(endpoints)

    def evolve_candidates(self, X, y, diversity: float):
        """
        â­ Step 4: Enhanced Evolution with Diversity Management

        ì „ëµ:
        1. Diversity ê¸°ë°˜ ì ì‘ì  ì§„í™”
        2. Tournament selection (ë” ê°•í•œ ì„ íƒì••)
        3. Elitism (ìµœê³ ë¥¼ ë³´ì¡´)
        4. Adaptive mutation rate
        """
        # Action ê¸°ì¤€ ì •ë ¬
        candidates_with_action = [
            (c.trajectory.action if c.trajectory else float('inf'), i)
            for i, c in enumerate(self.candidates)
        ]
        candidates_with_action.sort()

        # â­ Diversity ê¸°ë°˜ êµì²´ìœ¨ ì¡°ì •
        if diversity < 0.1:  # ë‹¤ì–‘ì„± ë‚®ìŒ â†’ ë” ë§ì´ êµì²´
            n_replace = max(2, self.n_candidates // 2)
        else:  # ë‹¤ì–‘ì„± ë†’ìŒ â†’ ì ê²Œ êµì²´
            n_replace = max(1, self.n_candidates // 3)

        # Elitism: ìµœê³ ëŠ” í•­ìƒ ë³´ì¡´
        best_indices = [idx for _, idx in candidates_with_action[:3]]
        worst_indices = [idx for _, idx in candidates_with_action[-n_replace:]]

        # â­ Adaptive mutation rate (diversity ê¸°ë°˜)
        mutation_rate = 0.2 if diversity < 0.15 else 0.1

        for i in worst_indices:
            if i in best_indices:  # ìµœê³ ëŠ” ë³´ì¡´
                continue

            if np.random.rand() < 0.7:
                # â­ Tournament selection (ë” ê°•í•œ ì„ íƒì••)
                tournament = np.random.choice(best_indices, size=2, replace=False)
                p1 = self.candidates[tournament[0]]
                p2 = self.candidates[tournament[1]]

                # Crossover with adaptive blend
                alpha = np.random.beta(2, 2)  # Beta distribution for blend
                child_endpoint = alpha * p1.best_endpoint + (1-alpha) * p2.best_endpoint
                child_endpoint += np.random.randn(len(child_endpoint)) * mutation_rate
            else:
                # Mutation from global best with exploration
                child_endpoint = self.global_best_endpoint.copy()
                child_endpoint += np.random.randn(len(child_endpoint)) * (mutation_rate * 2)

            self.candidates[i].predicted_endpoint = child_endpoint
            self.candidates[i].network.set_weights(child_endpoint)
            self.candidates[i].generation += 1

    def verify_least_action(self, candidate: EndpointCandidate) -> bool:
        """
        LAML í•µì‹¬: ìµœì†Œ ì‘ìš© ì›ë¦¬ ê²€ì¦ (ê°œì„ ëœ ë²„ì „)

        ê²½ë¡œê°€ ìµœì†Œ ì‘ìš©ì„ ë§Œì¡±í•˜ëŠ”ì§€ í™•ì¸
        """
        if candidate.trajectory is None:
            return False

        action = candidate.trajectory.action

        # ë™ì  threshold: ì „ì—­ ìµœì„  Action ê¸°ì¤€
        if self.global_best_action < float('inf'):
            # ì „ì—­ ìµœì„ ë³´ë‹¤ ì¢‹ê±°ë‚˜, ë¹„ìŠ·í•˜ë©´ (1.2ë°° ì´ë‚´) ìˆ˜ë½
            dynamic_threshold = self.global_best_action * 1.2
        else:
            # ì´ˆê¸°ì—ëŠ” ë§¤ìš° ê´€ëŒ€í•˜ê²Œ
            dynamic_threshold = float('inf')

        # ë˜ëŠ” ê°œì¸ ìµœì„ ë³´ë‹¤ ê°œì„ ë˜ì—ˆìœ¼ë©´ ìˆ˜ë½
        is_improvement = action < candidate.best_action

        return action < dynamic_threshold or is_improvement

    def refine_endpoint(self, candidate: EndpointCandidate, X, y):
        """
        LAML í•µì‹¬: ëì  ë³´ì •

        ìµœì†Œ ì‘ìš©ì„ ë§Œì¡±í•˜ì§€ ì•Šìœ¼ë©´ ëì ì„ ìˆ˜ì •
        """
        if candidate.trajectory is None:
            return

        # ë³´ì • ì „ëµë“¤
        strategies = []

        # 1. Gradient ë°©í–¥ìœ¼ë¡œ ì¡°ê¸ˆ ì´ë™
        temp_net = candidate.network.copy()
        temp_net.set_weights(candidate.predicted_endpoint)
        grad = temp_net.gradient(X, y)
        refined1 = candidate.predicted_endpoint - 0.1 * grad
        strategies.append(refined1)

        # 2. ì „ì—­ ìµœì„  ë°©í–¥ìœ¼ë¡œ ì´ë™
        direction = self.global_best_endpoint - candidate.predicted_endpoint
        refined2 = candidate.predicted_endpoint + 0.2 * direction
        strategies.append(refined2)

        # 3. ëœë¤ íƒìƒ‰
        refined3 = candidate.predicted_endpoint + np.random.randn(
            len(candidate.predicted_endpoint)) * self.temperature * 0.5
        strategies.append(refined3)

        # ê°€ì¥ ì¢‹ì€ ë³´ì • ì„ íƒ
        best_action = candidate.trajectory.action
        best_endpoint = candidate.predicted_endpoint

        for refined in strategies:
            candidate.predicted_endpoint = refined
            traj = candidate.compute_trajectory(X, y)
            if traj.action < best_action:
                best_action = traj.action
                best_endpoint = refined

        candidate.predicted_endpoint = best_endpoint

    def train(self, X, y, max_iters: int = 100, verbose: bool = True) -> Dict:
        """
        LAML-Q í•™ìŠµ

        ì•Œê³ ë¦¬ì¦˜:
        1. ê° í›„ë³´ê°€ ëì  ì˜ˆì¸¡
        2. ì‹œì‘â†’ë ê²½ë¡œ ìƒì„± (BVP)
        3. Action ê³„ì‚° ë° ìµœì†Œ ì‘ìš© ê²€ì¦
        4. ë¶ˆë§Œì¡±ì‹œ ëì  ë³´ì •
        5. ì§„í™”ì  ì„ íƒ
        6. ì˜¨ë„ ê°ì†Œ
        7. ìˆ˜ë ´ê¹Œì§€ ë°˜ë³µ
        """
        start_time = time.time()

        for iteration in range(max_iters):
            actions = []
            losses = []
            accepted = 0

            for candidate in self.candidates:
                # 1. ëì  ì˜ˆì¸¡ (LAML í•µì‹¬)
                candidate.predict_endpoint(
                    X, y,
                    self.temperature,
                    self.global_best_endpoint,
                    use_gradient_hint=True
                )

                # 2. ê²½ë¡œ ìƒì„± (BVP)
                trajectory = candidate.compute_trajectory(X, y)

                # 3. ìµœì†Œ ì‘ìš© ê²€ì¦ (LAML í•µì‹¬)
                if self.verify_least_action(candidate):
                    accepted += 1
                    candidate.update_best()

                    # â­ í•µì‹¬ ì¶”ê°€: ì‹¤ì œë¡œ ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸!
                    # ê¶¤ì ì„ ë”°ë¼ ì´ë™ (adaptive step)
                    current_w = candidate.network.get_weights()
                    direction = trajectory.end - current_w
                    # â­ Step 1: ê° candidateì˜ adaptive learning rate ì‚¬ìš©
                    step_size = candidate.learning_rate
                    new_w = current_w + step_size * direction
                    candidate.network.set_weights(new_w)

                    # ì˜ˆì¸¡ ëì ë„ ì—…ë°ì´íŠ¸
                    candidate.predicted_endpoint = new_w

                    # ì „ì—­ ìµœì„  ì—…ë°ì´íŠ¸
                    if trajectory.action < self.global_best_action:
                        self.global_best_action = trajectory.action
                        self.global_best_endpoint = trajectory.end.copy()
                        self.global_best_loss = trajectory.final_loss
                else:
                    # 4. ë¶ˆë§Œì¡±ì‹œ ë³´ì • (LAML í•µì‹¬)
                    self.refine_endpoint(candidate, X, y)

                    # ë¶ˆë§Œì¡±í•´ë„ ì¡°ê¸ˆì€ ì´ë™ (íƒìƒ‰)
                    current_w = candidate.network.get_weights()
                    grad = candidate.network.gradient(X, y)
                    new_w = current_w - 0.01 * grad  # ì‘ì€ SGD step
                    candidate.network.set_weights(new_w)

                actions.append(trajectory.action)
                losses.append(trajectory.final_loss)

            # 5. ì§„í™”ì  ì„ íƒ (QEDì—ì„œ) + Diversity ê´€ë¦¬
            if iteration % 5 == 0:
                diversity = self.get_diversity()
                self.evolve_candidates(X, y, diversity)

            # 6. ì˜¨ë„ ê°ì†Œ (íƒìƒ‰ â†’ ìˆ˜ë ´)
            self.temperature *= self.temp_decay

            # íˆìŠ¤í† ë¦¬ ê¸°ë¡
            acceptance_rate = accepted / self.n_candidates
            self.history['loss'].append(np.mean(losses))
            self.history['best_loss'].append(self.global_best_loss)
            self.history['action'].append(np.mean(actions))
            self.history['best_action'].append(self.global_best_action)
            self.history['diversity'].append(self.get_diversity())
            self.history['temperature'].append(self.temperature)
            self.history['acceptance_rate'].append(acceptance_rate)

            if verbose and iteration % 10 == 0:
                print(f"[{iteration:3d}] "
                      f"Loss: {np.mean(losses):.5f} | "
                      f"Best: {self.global_best_loss:.5f} | "
                      f"Action: {np.mean(actions):.3f} | "
                      f"Accept: {acceptance_rate:.1%} | "
                      f"Temp: {self.temperature:.3f}")

            # ì¡°ê¸° ì¢…ë£Œ
            if self.global_best_loss < 0.01:
                if verbose:
                    print(f"\nâœ“ Converged at iteration {iteration}")
                break

        elapsed = time.time() - start_time

        # â­ Step 3: Action-weighted Ensemble (ë‹¨ìˆœ í‰ê·  â†’ ê°€ì¤‘ í‰ê· )
        top_candidates = sorted(self.candidates,
                                key=lambda c: c.best_action)[:5]  # ìƒìœ„ 5ê°œ

        # Action ê¸°ë°˜ ê°€ì¤‘ì¹˜ ê³„ì‚° (ë‚®ì€ Action = ë†’ì€ ê°€ì¤‘ì¹˜)
        actions = np.array([c.best_action for c in top_candidates])
        weights = 1.0 / (actions + 1e-8)  # ì—­ìˆ˜
        weights = weights / weights.sum()  # ì •ê·œí™”

        # ê°€ì¤‘ í‰ê· 
        ensemble_weights = sum(w * c.best_endpoint
                              for w, c in zip(weights, top_candidates))

        return {
            'final_loss': self.global_best_loss,
            'final_action': self.global_best_action,
            'iterations': len(self.history['loss']),
            'time': elapsed,
            'best_weights': ensemble_weights,
            'acceptance_rate': np.mean(self.history['acceptance_rate'])
        }


# =============================================================================
# 4. SGD ë¹„êµìš©
# =============================================================================

class SGDOptimizer:
    """í‘œì¤€ SGD"""

    def __init__(self, network, learning_rate=0.1):
        self.net = network
        self.lr = learning_rate
        self.history = {'loss': []}

    def train(self, X, y, max_iters=100, verbose=False):
        start = time.time()
        for it in range(max_iters):
            grad = self.net.gradient(X, y)
            w = self.net.get_weights()
            w -= self.lr * grad
            self.net.set_weights(w)
            loss = self.net.loss(X, y)
            self.history['loss'].append(loss)
            if verbose and it % 10 == 0:
                print(f"[{it:3d}] Loss: {loss:.5f}")
            if loss < 0.01:
                break
        return {
            'final_loss': self.history['loss'][-1],
            'iterations': len(self.history['loss']),
            'time': time.time() - start
        }


# =============================================================================
# 5. ì‹¤í—˜
# =============================================================================

def make_dataset(name="nonlinear", n=100):
    """ë°ì´í„°ì…‹ ìƒì„±"""
    np.random.seed(42)
    X = np.random.randn(n, 4)

    if name == "linear":
        y = (X @ [1, -0.5, 0.3, 0.8]).reshape(-1, 1)
    elif name == "nonlinear":
        y = (np.sin(X[:, 0]) + np.cos(X[:, 1]) * X[:, 2]).reshape(-1, 1)
    elif name == "xor":
        y = ((X[:, 0] > 0) ^ (X[:, 1] > 0)).astype(float).reshape(-1, 1)

    X = (X - X.mean(0)) / (X.std(0) + 1e-8)
    y = (y - y.mean()) / (y.std() + 1e-8)
    return X, y


def run_experiment(dataset_name="nonlinear"):
    """LAML-Q vs SGD vs QED ë¹„êµ ì‹¤í—˜"""

    print(f"\n{'='*80}")
    print(f"ğŸ”¬ LAML-Q vs SGD: {dataset_name.upper()}")
    print(f"{'='*80}\n")

    X, y = make_dataset(dataset_name)

    # 1. LAML-Q
    print("1ï¸âƒ£  LAML-Q (Lagrangian Action Meta-Learning + Quantum)")
    print("-" * 80)
    net_laml = LightweightNN(seed=42)
    opt_laml = LAML_Q_Optimizer(
        net_laml,
        n_candidates=10,
        temperature_init=0.3,
        temperature_decay=0.97,
        action_threshold=0.5
    )
    result_laml = opt_laml.train(X, y, max_iters=100, verbose=True)

    # 2. SGD
    print(f"\n2ï¸âƒ£  Standard SGD")
    print("-" * 80)
    net_sgd = LightweightNN(seed=42)
    opt_sgd = SGDOptimizer(net_sgd, learning_rate=0.1)
    result_sgd = opt_sgd.train(X, y, max_iters=100, verbose=True)

    # ê²°ê³¼ ë¹„êµ
    print(f"\n{'='*80}")
    print("ğŸ“Š ê²°ê³¼")
    print(f"{'='*80}")
    print(f"{'ì§€í‘œ':<25} {'LAML-Q':>25} {'SGD':>20}")
    print("-" * 80)
    print(f"{'ìµœì¢… ì†ì‹¤':<25} {result_laml['final_loss']:>25.6f} {result_sgd['final_loss']:>20.6f}")
    print(f"{'ìˆ˜ë ´ ë°˜ë³µ':<25} {result_laml['iterations']:>25d} {result_sgd['iterations']:>20d}")
    print(f"{'ì‹œê°„ (ì´ˆ)':<25} {result_laml['time']:>25.4f} {result_sgd['time']:>20.4f}")
    print(f"{'ìˆ˜ë½ë¥ ':<25} {result_laml['acceptance_rate']:>25.1%} {'N/A':>20}")

    improvement = (result_sgd['final_loss'] - result_laml['final_loss']) / result_sgd['final_loss'] * 100

    if improvement > 0:
        print(f"\nğŸ¯ LAML-Q ê°œì„ ìœ¨: +{improvement:.2f}%")
        print("=" * 80)
        print("âœ… LAML-Q ìŠ¹ë¦¬!")
        winner = "LAML-Q"
    else:
        print(f"\nâŒ SGD ëŒ€ë¹„ {-improvement:.2f}% ì—´ë“±")
        print("=" * 80)
        print("SGD ìŠ¹ë¦¬")
        winner = "SGD"

    return {
        'dataset': dataset_name,
        'laml_q': result_laml,
        'sgd': result_sgd,
        'laml_q_opt': opt_laml,
        'sgd_opt': opt_sgd,
        'improvement': improvement,
        'winner': winner
    }


def plot_results(results):
    """ì‹œê°í™”"""
    laml = results['laml_q_opt']
    sgd = results['sgd_opt']
    ds = results['dataset']

    fig, axes = plt.subplots(2, 3, figsize=(18, 10))
    fig.suptitle(f"LAML-Q vs SGD: {ds.upper()}\n"
                 f"LAML-Q: {results['laml_q']['final_loss']:.5f} | "
                 f"SGD: {results['sgd']['final_loss']:.5f} | "
                 f"Winner: {results['winner']}",
                 fontsize=16, fontweight='bold')

    # 1. Loss ë¹„êµ
    ax = axes[0, 0]
    ax.plot(laml.history['best_loss'], 'b-', label='LAML-Q (Best)', linewidth=2.5)
    ax.plot(laml.history['loss'], 'b--', label='LAML-Q (Avg)', alpha=0.6)
    ax.plot(sgd.history['loss'], 'r-', label='SGD', linewidth=2.5)
    ax.set_xlabel('Iteration')
    ax.set_ylabel('Loss')
    ax.set_title('Learning Curves', fontweight='bold')
    ax.legend()
    ax.grid(True, alpha=0.3)
    ax.set_yscale('log')

    # 2. Action (LAML í•µì‹¬ ì§€í‘œ)
    ax = axes[0, 1]
    ax.plot(laml.history['best_action'], 'g-', linewidth=2.5, label='Best Action')
    ax.plot(laml.history['action'], 'g--', alpha=0.6, label='Avg Action')
    ax.set_xlabel('Iteration')
    ax.set_ylabel('Action S[Î¸]')
    ax.set_title('Lagrangian Action (LAML Core)', fontweight='bold')
    ax.legend()
    ax.grid(True, alpha=0.3)

    # 3. ìˆ˜ë½ë¥  (ìµœì†Œ ì‘ìš© ê²€ì¦)
    ax = axes[0, 2]
    ax.plot(laml.history['acceptance_rate'], 'm-', linewidth=2.5)
    ax.set_xlabel('Iteration')
    ax.set_ylabel('Acceptance Rate')
    ax.set_title('Least Action Verification', fontweight='bold')
    ax.grid(True, alpha=0.3)
    ax.set_ylim(0, 1)

    # 4. ë‹¤ì–‘ì„±
    ax = axes[1, 0]
    ax.plot(laml.history['diversity'], 'c-', linewidth=2.5)
    ax.set_xlabel('Iteration')
    ax.set_ylabel('Diversity')
    ax.set_title('Endpoint Diversity', fontweight='bold')
    ax.grid(True, alpha=0.3)

    # 5. ì˜¨ë„
    ax = axes[1, 1]
    ax.plot(laml.history['temperature'], 'orange', linewidth=2.5)
    ax.set_xlabel('Iteration')
    ax.set_ylabel('Temperature')
    ax.set_title('Exploration Temperature', fontweight='bold')
    ax.grid(True, alpha=0.3)

    # 6. ìµœì¢… ë¹„êµ
    ax = axes[1, 2]
    methods = ['LAML-Q', 'SGD']
    losses = [results['laml_q']['final_loss'], results['sgd']['final_loss']]
    colors = ['blue', 'red']
    bars = ax.bar(methods, losses, color=colors, alpha=0.8)
    ax.set_ylabel('Final Loss')
    ax.set_title('Final Performance', fontweight='bold')
    for bar, loss in zip(bars, losses):
        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                f'{loss:.5f}', ha='center', fontsize=12)
    ax.grid(True, alpha=0.3, axis='y')

    plt.tight_layout()
    filename = f"/Users/say/Documents/GitHub/ai/laml_q_{ds}_results.png"
    plt.savefig(filename, dpi=300, bbox_inches='tight')
    print(f"\nâœ… ì €ì¥: {filename}")
    plt.close()


# =============================================================================
# 6. ë©”ì¸
# =============================================================================

if __name__ == "__main__":
    print("\n" + "=" * 80)
    print("ğŸŒŸ LAML-Q: Lagrangian Action Meta-Learning with Quantum-inspired Ensemble")
    print("   LAML + QED + GENESIS ì™„ì „ í†µí•©")
    print("=" * 80)
    print("\ní•µì‹¬ ì² í•™ (LAMLì—ì„œ ê³„ìŠ¹):")
    print("  1. ëì (ìµœì¢… ê°€ì¤‘ì¹˜) ì˜ˆì¸¡")
    print("  2. ì‹œì‘â†’ë ê²½ë¡œ ì—­ì‚° (BVP)")
    print("  3. ìµœì†Œ ì‘ìš© ì›ë¦¬ë¡œ ê²½ë¡œ ê²€ì¦")
    print("  4. ë¶ˆë§Œì¡±ì‹œ ë³´ì • ë°˜ë³µ")
    print("\ní•´ê²°ì±… (QED/GENESISì—ì„œ):")
    print("  1. ë‹¨ì¼ ì˜ˆì¸¡ â†’ Nê°œ ëì  í›„ë³´ ì•™ìƒë¸”")
    print("  2. ì§„í™”ì  ì„ íƒìœ¼ë¡œ ì¢‹ì€ í›„ë³´ ê°•í™”")
    print("  3. ì–‘ì í„°ë„ë§ìœ¼ë¡œ íƒìƒ‰ ê°•í™”")
    print("  4. ìê¸°ì¡°ì§í™”ë¡œ íƒìƒ‰/ìˆ˜ë ´ ê· í˜•")

    datasets = ["linear", "nonlinear", "xor"]
    all_results = {}
    wins = 0

    for ds in datasets:
        result = run_experiment(ds)
        all_results[ds] = result
        plot_results(result)
        if result['winner'] == "LAML-Q":
            wins += 1

    # ìµœì¢… ìš”ì•½
    print(f"\n{'='*80}")
    print("ğŸ¯ ìµœì¢… ìš”ì•½: LAML-Q vs SGD")
    print(f"{'='*80}")

    for ds, res in all_results.items():
        status = "âœ… LAML-Q" if res['winner'] == "LAML-Q" else "âŒ SGD"
        print(f"{ds.upper():12s} | {status:15s} | {res['improvement']:+7.2f}%")

    print(f"\n{'='*80}")
    print(f"LAML-Q ìŠ¹ë¥ : {wins}/{len(datasets)} ({wins/len(datasets)*100:.1f}%)")
    print(f"{'='*80}")

    if wins == len(datasets):
        print("\nğŸ‰ LAML ì² í•™ì´ ì‹¤ì¦ì ìœ¼ë¡œ ì¦ëª…ë˜ì—ˆìŠµë‹ˆë‹¤!")
        print("   - ëì  ì˜ˆì¸¡ â†’ ê²½ë¡œ ì—­ì‚° â†’ ìµœì†Œ ì‘ìš© ê²€ì¦ â†’ ë³´ì •")
        print("   - ì´ íŒ¨ëŸ¬ë‹¤ì„ì´ SGDë¥¼ ëª¨ë“  ë°ì´í„°ì…‹ì—ì„œ ë›°ì–´ë„˜ì—ˆìŠµë‹ˆë‹¤!")
    elif wins > 0:
        print(f"\nâš¡ LAML ì² í•™ì´ ë¶€ë¶„ì ìœ¼ë¡œ ì¦ëª…ë˜ì—ˆìŠµë‹ˆë‹¤! ({wins}/{len(datasets)})")
    else:
        print("\nâš ï¸ ì¶”ê°€ ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤.")
